{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import multiprocessing as mp\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from typing import List\n",
    "\n",
    "parser = argparse.ArgumentParser(description='preprocess')\n",
    "parser.add_argument('--task', default='fb15k237_owe', type=str, metavar='N',\n",
    "                    help='dataset name')\n",
    "parser.add_argument('--workers', default=8, type=int, metavar='N',\n",
    "                    help='number of workers')\n",
    "parser.add_argument('--train-path', default='/mnt/data/data/home/liuyafei/论文2/temp/SimKGC-prompt/data/FB15k-237-OWE/train.txt', type=str, metavar='N',\n",
    "                    help='path to training data')\n",
    "parser.add_argument('--valid-path', default='/mnt/data/data/home/liuyafei/论文2/temp/SimKGC-prompt/data/FB15k-237-OWE/valid.txt', type=str, metavar='N',\n",
    "                    help='path to valid data')\n",
    "parser.add_argument('--test-path', default='/mnt/data/data/home/liuyafei/论文2/temp/SimKGC-prompt/data/FB15k-237-OWE/test_zero.txt', type=str, metavar='N',\n",
    "                    help='path to valid data')\n",
    "\n",
    "args = parser.parse_args()\n",
    "mp.set_start_method('fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_relations(examples: List[dict], normalize_fn, is_train: bool):\n",
    "    relation_id_to_str = {}\n",
    "    for ex in examples:\n",
    "        rel_str = normalize_fn(ex['relation'])\n",
    "        relation_id_to_str[ex['relation']] = rel_str\n",
    "        ex['relation'] = rel_str\n",
    "\n",
    "    _check_sanity(relation_id_to_str)\n",
    "\n",
    "    if is_train:\n",
    "        out_path = '{}/relations.json'.format(os.path.dirname(args.train_path))\n",
    "        with open(out_path, 'w', encoding='utf-8') as writer:\n",
    "            json.dump(relation_id_to_str, writer, ensure_ascii=False, indent=4)\n",
    "            print('Save {} relations to {}'.format(len(relation_id_to_str), out_path))\n",
    "\n",
    "def _check_sanity(relation_id_to_str: dict):\n",
    "    # We directly use normalized relation string as a key for training and evaluation,\n",
    "    # make sure no two relations are normalized to the same surface form\n",
    "    relation_str_to_id = {}\n",
    "    for rel_id, rel_str in relation_id_to_str.items():\n",
    "        if rel_str is None:\n",
    "            continue\n",
    "        if rel_str not in relation_str_to_id:\n",
    "            relation_str_to_id[rel_str] = rel_id\n",
    "        elif relation_str_to_id[rel_str] != rel_id:\n",
    "            assert False, 'ERROR: {} and {} are both normalized to {}'\\\n",
    "                .format(relation_str_to_id[rel_str], rel_id, rel_str)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局字典\n",
    "fb15k_id2ent = {}\n",
    "fb15k_id2desc = {}\n",
    "\n",
    "def _load_fb15k237_data_from_json(json_path: str):\n",
    "    \"\"\"Load data from the entity2wikidata.json file.\"\"\"\n",
    "    global fb15k_id2desc, fb15k_id2ent\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for entity_id, content in data.items():\n",
    "        # 更新描述字典\n",
    "        fb15k_id2desc[entity_id] = content.get('description', '')\n",
    "        # 更新实体字典\n",
    "        fb15k_id2ent[entity_id] = (\n",
    "            entity_id, \n",
    "            content.get('label', '').replace('_', ' ').strip(), \n",
    "            content.get('description', '')\n",
    "        )\n",
    "\n",
    "def _process_line_fb15k237(line: str) -> dict:\n",
    "    \"\"\"Process a single line from the dataset.\"\"\"\n",
    "    fs = line.strip().split('\\t')\n",
    "    assert len(fs) == 3, 'Expect 3 fields for {}'.format(line)\n",
    "    head_id, relation, tail_id = fs[0], fs[1], fs[2]\n",
    "\n",
    "    _, head, _ = fb15k_id2ent.get(head_id, (head_id, '', ''))\n",
    "    _, tail, _ = fb15k_id2ent.get(tail_id, (tail_id, '', ''))\n",
    "    example = {\n",
    "        'head_id': head_id,\n",
    "        'head': head,\n",
    "        'relation': relation,\n",
    "        'tail_id': tail_id,\n",
    "        'tail': tail\n",
    "    }\n",
    "    return example\n",
    "   \n",
    "def preprocess_fb15k237(path):\n",
    "    \"\"\"Preprocess FB15k-237 dataset.\"\"\"\n",
    "    if not fb15k_id2desc or not fb15k_id2ent:\n",
    "        _load_fb15k237_data_from_json(json_path)\n",
    "\n",
    "    lines = open(path, 'r', encoding='utf-8').readlines()\n",
    "    pool = Pool(processes=args.workers)\n",
    "    examples = pool.map(_process_line_fb15k237, lines)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    _normalize_relations(examples, normalize_fn=_normalize_fb15k237_relation, is_train=(path == args.train_path))\n",
    "\n",
    "    out_path = path + '.json'\n",
    "    json.dump(examples, open(out_path, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "    print('Save {} examples to {}'.format(len(examples), out_path))\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fb15k_id2ent = {}\n",
    "fb15k_id2desc = {}\n",
    "\n",
    "def _load_fb15k237_desc(path: str):\n",
    "    global fb15k_id2desc\n",
    "    lines = open(path, 'r', encoding='utf-8').readlines()\n",
    "    for line in lines:\n",
    "        fs = line.strip().split('\\t')\n",
    "        assert len(fs) == 2, 'Invalid line: {}'.format(line.strip())\n",
    "        entity_id, desc = fs[0], fs[1]\n",
    "        fb15k_id2desc[entity_id] = _truncate(desc, 50)\n",
    "    print('Load {} entity descriptions from {}'.format(len(fb15k_id2desc), path))\n",
    "\n",
    "def _load_fb15k237_wikidata(path: str):\n",
    "    global fb15k_id2ent, fb15k_id2desc\n",
    "    lines = open(path, 'r', encoding='utf-8').readlines()\n",
    "    for line in lines:\n",
    "        fs = line.strip().split('\\t')\n",
    "        assert len(fs) == 2, 'Invalid line: {}'.format(line.strip())\n",
    "        entity_id, name = fs[0], fs[1]\n",
    "        name = name.replace('_', ' ').strip()\n",
    "        if entity_id not in fb15k_id2desc:\n",
    "            print('No desc found for {}'.format(entity_id))\n",
    "        fb15k_id2ent[entity_id] = (entity_id, name, fb15k_id2desc.get(entity_id, ''))\n",
    "    print('Load {} entity names from {}'.format(len(fb15k_id2ent), path))\n",
    " \n",
    " def _process_line_fb15k237(line: str) -> dict:\n",
    "    fs = line.strip().split('\\t')\n",
    "    assert len(fs) == 3, 'Expect 3 fields for {}'.format(line)\n",
    "    head_id, relation, tail_id = fs[0], fs[1], fs[2]\n",
    "\n",
    "    _, head, _ = fb15k_id2ent[head_id]\n",
    "    _, tail, _ = fb15k_id2ent[tail_id]\n",
    "    example = {'head_id': head_id,\n",
    "               'head': head,\n",
    "               'relation': relation,\n",
    "               'tail_id': tail_id,\n",
    "               'tail': tail}\n",
    "    return example\n",
    "   \n",
    "def preprocess_fb15k237(path):\n",
    "    if not fb15k_id2desc:\n",
    "        _load_fb15k237_desc('{}/FB15k_mid2description.txt'.format(os.path.dirname(path)))\n",
    "    if not fb15k_id2ent:\n",
    "        _load_fb15k237_wikidata('{}/FB15k_mid2name.txt'.format(os.path.dirname(path)))\n",
    "\n",
    "    lines = open(path, 'r', encoding='utf-8').readlines()\n",
    "    pool = Pool(processes=args.workers)\n",
    "    examples = pool.map(_process_line_fb15k237, lines)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "all_examples = []\n",
    "for path in [args.train_path, args.valid_path, args.test_path]:\n",
    "    all_examples += preprocess_fb15k237(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_all_entities(examples, out_path, id2text: dict):\n",
    "    id2entity = {}\n",
    "    relations = set()\n",
    "    for ex in examples:\n",
    "        head_id = ex['head_id']\n",
    "        relations.add(ex['relation'])\n",
    "        if head_id not in id2entity:\n",
    "            # Check if the key exists in id2text before accessing it\n",
    "            if head_id in id2text:\n",
    "                id2entity[head_id] = {\n",
    "                    'entity_id': head_id,\n",
    "                    'entity': ex['head'],\n",
    "                    'entity_desc': id2text[head_id]\n",
    "                }\n",
    "\n",
    "        tail_id = ex['tail_id']\n",
    "        if tail_id not in id2entity:\n",
    "            # Similarly, check if the key exists for tail_id\n",
    "            if tail_id in id2text:\n",
    "                id2entity[tail_id] = {\n",
    "                    'entity_id': tail_id,\n",
    "                    'entity': ex['tail'],\n",
    "                    'entity_desc': id2text[tail_id]\n",
    "                }\n",
    "        # if head_id not in id2entity:\n",
    "        #     id2entity[head_id] = {'entity_id': head_id,\n",
    "        #                           'entity': ex['head'],\n",
    "        #                           'entity_desc': id2text[head_id]}\n",
    "        # tail_id = ex['tail_id']\n",
    "        # if tail_id not in id2entity:\n",
    "        #     id2entity[tail_id] = {'entity_id': tail_id,\n",
    "        #                           'entity': ex['tail'],\n",
    "        #                           'entity_desc': id2text[tail_id]}\n",
    "    print('Get {} entities, {} relations in total'.format(len(id2entity), len(relations)))\n",
    "\n",
    "    json.dump(list(id2entity.values()), open(out_path, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples = []\n",
    "for path in [args.train_path, args.valid_path, args.test_path]:\n",
    "    assert os.path.exists(path)\n",
    "    print('Process {}...'.format(path))\n",
    "    if args.task.lower() == 'wn18rr':\n",
    "        all_examples += preprocess_wn18rr(path)\n",
    "    elif args.task.lower() == 'fb15k237':\n",
    "        all_examples += preprocess_fb15k237(path)\n",
    "    elif args.task.lower() in ['wiki5m_trans', 'wiki5m_ind']:\n",
    "        all_examples += preprocess_wiki5m(path, is_train=(path == args.train_path))\n",
    "    else:\n",
    "        assert False, 'Unknown task: {}'.format(args.task)\n",
    "\n",
    "if args.task.lower() == 'wn18rr':\n",
    "    id2text = {k: v[2] for k, v in wn18rr_id2ent.items()}\n",
    "elif args.task.lower() == 'fb15k237':\n",
    "    id2text = {k: v[2] for k, v in fb15k_id2ent.items()}\n",
    "elif args.task.lower() in ['wiki5m_trans', 'wiki5m_ind']:\n",
    "    id2text = wiki5m_id2text\n",
    "else:\n",
    "    assert False, 'Unknown task: {}'.format(args.task)\n",
    "\n",
    "dump_all_entities(all_examples,\n",
    "                    out_path='{}/entities.json'.format(os.path.dirname(args.train_path)),\n",
    "                    id2text=id2text)\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
